{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "### Intersection over Union (IoU)\n",
    "\n",
    "Intersection over Union (IoU) is a metric used in object detection tasks to evaluate the overlap between two bounding boxes. It is calculated using the following formula:\n",
    "\n",
    "$$\n",
    "IoU = \\frac{{\\text{Intersection Area}}}{{\\text{Union Area}}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- Intersection Area (IA) is the area where the two bounding boxes overlap.\n",
    "- Union Area (UA) is the total area covered by both bounding boxes.\n",
    "\n",
    "IoU ranges from 0 to 1, with higher values indicating greater overlap between the bounding boxes.\n",
    "\n",
    "![](images/IoU_drawing1.jpg)\n",
    "![](images/IoU_drawing2.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "**Precision**: Ratio of true positive predictions to total positive predictions made by the model. It measures the accuracy of positive predictions:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "$$\n",
    "\n",
    "**Recall**: Ratio of true positive predictions to all actual positive instances in the dataset. It indicates the model's ability to identify positive instances:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "$$\n",
    "\n",
    "- **True Positive (TP)**: Represents the instances that are correctly predicted as positive by the model. For example, in a medical diagnosis scenario, a true positive would occur when the model correctly identifies a patient with a disease as having the disease.\n",
    "\n",
    "- **False Positive (FP)**: Denotes the instances that are incorrectly predicted as positive by the model. In other words, these are instances where the model predicts a positive outcome when it should have predicted a negative outcome. For instance, in the medical diagnosis example, a false positive would happen if the model incorrectly labels a healthy patient as having the disease.\n",
    "\n",
    "## task 1c)\n",
    "\n",
    "1. Calculate Average Precision (AP) for each class using the precision and recall values provided, using the trapezoidal rule:\n",
    "\n",
    "$$\n",
    "\\text{AP}_i = \\int_{0}^{1} \\text{precision}_i(r) \\, \\text{d}r\n",
    "$$\n",
    "\n",
    "where $\\text{precision}_i(r)$ is the precision for class $i$ at recall level $r$.\n",
    "\n",
    "2. Compute Mean Average Precision (mAP) by taking the average of AP values for all classes:\n",
    "\n",
    "$$\n",
    "\\text{mAP} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{AP}_i\n",
    "$$\n",
    "\n",
    "where $N$ is the number of classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (mAP) for Class 1: 0.6799999999999999\n",
      "Mean Average Precision (mAP) for Class 2: 0.375\n",
      "Mean Average Precision (mAP) across both classes: 0.5275\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_average_precision(precision, recall):\n",
    "    mAP = np.trapz(precision, recall)\n",
    "    return mAP\n",
    "\n",
    "# Precision and recall curve for class 1\n",
    "precision1 = [1.0, 1.0, 1.0, 0.5, 0.20]\n",
    "recall1 = [0.05, 0.1, 0.4, 0.7, 1.0]\n",
    "\n",
    "# Precision and recall curve for class 2\n",
    "precision2 = [1.0, 0.80, 0.60, 0.5, 0.20]\n",
    "recall2 = [0.3, 0.4, 0.5, 0.7, 1.0]\n",
    "\n",
    "# Calculate mean average precision (mAP) for each class\n",
    "mAP_class1 = np.trapz(precision1, recall1)\n",
    "mAP_class2 = np.trapz(precision2, recall2)\n",
    "\n",
    "# Calculate mean average precision (mAP) across both classes\n",
    "mAP = np.mean([mAP_class1, mAP_class2])\n",
    "\n",
    "print(\"Mean Average Precision (mAP) for Class 1:\", mAP_class1)\n",
    "print(\"Mean Average Precision (mAP) for Class 2:\", mAP_class2)\n",
    "print(\"Mean Average Precision (mAP) across both classes:\", mAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "Fill in task 1a image of hand-written notes which are easy to read, or latex equations here\n",
    "\n",
    "### Task 3b)\n",
    "Fill in task 1a image of hand-written notes which are easy to read, or latex equations here\n",
    "\n",
    "### Task 3c)\n",
    "Fill in task 1a image of hand-written notes which are easy to read, or latex equations here\n",
    "\n",
    "\n",
    "### Task 3d)\n",
    "Fill in task 1a image of hand-written notes which are easy to read, or latex equations here\n",
    "\n",
    "### Task 3e)\n",
    "Fill in task 1a image of hand-written notes which are easy to read, or latex equations here\n",
    "\n",
    "### Task 3f)\n",
    "Fill in task 1a image of hand-written notes which are easy to read, or latex equations here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "\n",
    "FILL IN ANSWER. \n",
    "\n",
    "## Task 4c)\n",
    "FILL IN ANSWER. \n",
    "\n",
    "\n",
    "## Task 4d)\n",
    "FILL IN ANSWER. \n",
    "\n",
    "\n",
    "## Task 4e)\n",
    "FILL IN ANSWER. \n",
    "\n",
    "\n",
    "## Task 4f)\n",
    "FILL IN ANSWER. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdt4265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
