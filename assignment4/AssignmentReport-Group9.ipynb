{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "### Intersection over Union (IoU)\n",
    "\n",
    "Intersection over Union (IoU) is a metric used in object detection tasks to evaluate the overlap between two bounding boxes. It is calculated using the following formula:\n",
    "\n",
    "$$\n",
    "IoU = \\frac{{\\text{Intersection Area}}}{{\\text{Union Area}}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- Intersection Area (IA) is the area where the two bounding boxes overlap.\n",
    "- Union Area (UA) is the total area covered by both bounding boxes.\n",
    "\n",
    "IoU ranges from 0 to 1, with higher values indicating greater overlap between the bounding boxes.\n",
    "\n",
    "![](images/IoU_drawing1.jpg)\n",
    "![](images/IoU_drawing2.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "**Precision**: Ratio of true positive predictions to total positive predictions made by the model. It measures the accuracy of positive predictions:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "$$\n",
    "\n",
    "**Recall**: Ratio of true positive predictions to all actual positive instances in the dataset. It indicates the model's ability to identify positive instances:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "$$\n",
    "\n",
    "- **True Positive (TP)**: Represents the instances that are correctly predicted as positive by the model. For example, in a medical diagnosis scenario, a true positive would occur when the model correctly identifies a patient with a disease as having the disease.\n",
    "\n",
    "- **False Positive (FP)**: Denotes the instances that are incorrectly predicted as positive by the model. In other words, these are instances where the model predicts a positive outcome when it should have predicted a negative outcome. For instance, in the medical diagnosis example, a false positive would happen if the model incorrectly labels a healthy patient as having the disease.\n",
    "\n",
    "## task 1c)\n",
    "\n",
    "1. Calculate Average Precision (AP) for each class using the precision and recall values provided, using the trapezoidal rule:\n",
    "\n",
    "$$\n",
    "\\text{AP}_i = \\int_{0}^{1} \\text{precision}_i(r) \\, \\text{d}r\n",
    "$$\n",
    "\n",
    "where $\\text{precision}_i(r)$ is the precision for class $i$ at recall level $r$.\n",
    "\n",
    "2. Compute Mean Average Precision (mAP) by taking the average of AP values for all classes:\n",
    "\n",
    "$$\n",
    "\\text{mAP} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{AP}_i\n",
    "$$\n",
    "\n",
    "where $N$ is the number of classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (mAP) for Class 1: 0.6799999999999999\n",
      "Mean Average Precision (mAP) for Class 2: 0.375\n",
      "Mean Average Precision (mAP) across both classes: 0.5275\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_average_precision(precision, recall):\n",
    "    mAP = np.trapz(precision, recall)\n",
    "    return mAP\n",
    "\n",
    "# Precision and recall curve for class 1\n",
    "precision1 = [1.0, 1.0, 1.0, 0.5, 0.20]\n",
    "recall1 = [0.05, 0.1, 0.4, 0.7, 1.0]\n",
    "\n",
    "# Precision and recall curve for class 2\n",
    "precision2 = [1.0, 0.80, 0.60, 0.5, 0.20]\n",
    "recall2 = [0.3, 0.4, 0.5, 0.7, 1.0]\n",
    "\n",
    "# Calculate mean average precision (mAP) for each class\n",
    "mAP_class1 = np.trapz(precision1, recall1)\n",
    "mAP_class2 = np.trapz(precision2, recall2)\n",
    "\n",
    "# Calculate mean average precision (mAP) across both classes\n",
    "mAP = np.mean([mAP_class1, mAP_class2])\n",
    "\n",
    "print(\"Mean Average Precision (mAP) for Class 1:\", mAP_class1)\n",
    "print(\"Mean Average Precision (mAP) for Class 2:\", mAP_class2)\n",
    "print(\"Mean Average Precision (mAP) across both classes:\", mAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "### Understanding the Precision-Recall Curve\n",
    "\n",
    "This plot shows how well our model identifies true positives (correct predictions) while avoiding false positives (incorrect positive predictions).\n",
    "\n",
    "- **Early Performance:** Initially, the model does exceptionally well; it accurately predicts most of the positive cases without making many mistakes. This is why we see high precision even as the model identifies a larger percentage of the true positives, which is what recall measures.\n",
    "\n",
    "- **Decline in Precision:** As the recall approaches 1 (meaning the model tries to identify all true positives), there's a sharp drop in precision. This indicates that to find all positives, the model starts to mislabel more negative cases as positive, leading to more errors.\n",
    "\n",
    "- **Analyzing Results:** The model is highly effective up to a recall of about 0.9, maintaining accuracy while catching most positives. Past this point, trying to catch every single positive results in a significant increase in false positives. For practical use, it suggests that we might want to set a threshold that balances recall and precision before this sharp decline, to keep both false positives and false negatives at acceptable levels for our specific application.\n",
    "\n",
    "![Precision recall curve](precision_recall_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "The filtering operation used to remove overlapping bounding boxes in SSD during inference is called **Non-Maximum Suppression (NMS)**.\n",
    "\n",
    "### Task 3b)\n",
    "**False.** In the SSD architecture, predictions from the deeper layers are responsible for detecting larger objects. It's the predictions from the earlier layers that are responsible for detecting smaller objects.\n",
    "\n",
    "### Task 3c)\n",
    "They use different shapes (aspect ratios) for the bounding boxes at the same spot on the image to better identify objects that look differentâ€”like tall trees or wide cars. This helps the SSD predict what the object is (class scores) and how to frame it accurately (by adjusting four key measurements from the original box), improving its ability to recognize and locate diverse objects within the scene.\n",
    "\n",
    "### Task 3d)\n",
    "The main difference between SSD and YOLOv1/v2 is that SSD detects objects across multiple scales using different-sized feature maps, while YOLOv1/v2 typically uses a single-scale feature map. This makes SSD more effective at identifying objects of various sizes, especially smaller ones.\n",
    "\n",
    "**SSD:**\n",
    "\n",
    "Pros:\n",
    "- Better at detecting small objects due to its use of multiple feature maps.\n",
    "- Generally provides high accuracy.\n",
    "\n",
    "Cons:\n",
    "- More complex and can be slightly slower than YOLO due to its detailed multi-scale approach.\n",
    "\n",
    "**YOLOv1/v2:**\n",
    "\n",
    "Pros:\n",
    "- Extremely fast, making it ideal for real-time applications.\n",
    "- Simpler architecture due to its single-scale, single-shot approach.\n",
    "\n",
    "Cons:\n",
    "- Less effective at detecting small objects compared to SSD.\n",
    "- Lower localization accuracy, leading to less precise object detection.\n",
    "\n",
    "\n",
    "\n",
    "### Task 3e)\n",
    "To calculate the total number of anchor boxes for the given SSD framework with a feature map resolution of $38 \\times 38$ and 6 different aspect ratios per anchor location, we use:\n",
    "\n",
    "$$\n",
    "\\text{Total number of anchors} = \\text{Number of anchor locations} \\times \\text{Number of aspect ratios}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Number of anchor locations} = \\text{Height} \\times \\text{Width}\n",
    "$$\n",
    "\n",
    "For a $38 \\times 38$ feature map:\n",
    "$$\n",
    "\\text{Number of anchor locations} = 38 \\times 38 = 1444\n",
    "$$\n",
    "\n",
    "Now, since we have 6 different aspect ratios for each anchor location, the total number of anchor boxes will be:\n",
    "\n",
    "$$\n",
    "\\text{Total number of anchors} = 1444 \\times 6 = 8664\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Task 3f)\n",
    "The total number of anchor boxes for the entire network can be calculated by summing up the anchor boxes for each feature map: \n",
    "\n",
    "Each feature map has **6** aspect ratios.\n",
    "\n",
    "$38 \\times 38 \\times 6 = 8664$ anchor boxes\n",
    "\n",
    "$19 \\times 19 \\times 6 = 2166$ anchor boxes\n",
    "\n",
    "$10 \\times 10 \\times 6 = 600$ anchor boxes\n",
    "\n",
    "$5 \\times 5 \\times 6 = 150$ anchor boxes\n",
    "\n",
    "$3 \\times 3 \\times 6 = 54$ anchor boxes\n",
    "\n",
    "$1 \\times 1 \\times 6 = 6$ anchor boxes\n",
    "\n",
    "$Sum = 8664 + 2166 + 600 + 150 + 54 + 6 = 11640$ **anchor boxes**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11640"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define resolutions of feature maps\n",
    "resolutions = [(38, 38), (19, 19), (10, 10), (5, 5), (3, 3), (1, 1)]\n",
    "aspect_ratios = 6\n",
    "\n",
    "# Calculate total number of anchor boxes\n",
    "total_anchors = 0\n",
    "for resolution in resolutions:\n",
    "    height, width = resolution\n",
    "    total_anchors += height * width * aspect_ratios\n",
    "\n",
    "total_anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "\n",
    "FILL IN ANSWER. \n",
    "\n",
    "## Task 4c)\n",
    "FILL IN ANSWER. \n",
    "\n",
    "\n",
    "## Task 4d)\n",
    "FILL IN ANSWER. \n",
    "\n",
    "\n",
    "## Task 4e)\n",
    "FILL IN ANSWER. \n",
    "\n",
    "\n",
    "## Task 4f)\n",
    "FILL IN ANSWER. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdt4265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
